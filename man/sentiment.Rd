% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/sentiment.R, R/sentimentr-package.R
\docType{package}
\name{sentiment}
\alias{package-sentiment}
\alias{sentiment}
\alias{sentiment-package}
\title{Polarity Score (Sentiment Analysis)}
\usage{
sentiment(text.var, polarity_dt = sentimentr::polarity_table,
  valence_shifters_dt = sentimentr::valence_shifters_table,
  amplifier.weight = 0.8, n.before = 4, n.after = 2,
  question.weight = 1, ...)
}
\arguments{
\item{text.var}{The text variable.}

\item{polarity_dt}{A \pkg{data.table} of positive/negative words and
weights with x and y as column names..}

\item{valence_shifters_dt}{A \pkg{data.table} of valence shifters that
can alter a polarized word's meaning and a numic key for negators (1),
amplifiers(2), and de-amplifiers (3) with x and y as column names.}

\item{amplifier.weight}{The weight to apply to amplifiers/de-amplifiers (values
from 0 to 1).  This value will multiply the polarized terms by 1 + this
value.}

\item{n.before}{The number of words to consider as valence shifters before
the polarized word.}

\item{n.after}{The number of words to consider as valence shifters after
the polarized word.}

\item{question.weight}{The weighting of questions (values from 0 to 1).
Default is 1.  A 0 corresponds with the belief that questions (pure questions)
are not polarized.  A weight may be applied based on the evidence that the
questions function with polarized sentiment.}

\item{\ldots}{Ignored.}
}
\value{
Returns a \pkg{data.table} of:
\itemize{
  \item  element_id - The id number of the original vector passed to \code{sentiment}
  \item  sentence_id - The id number of the sentences within each \code{element_id}
  \item  word_count - Word count
  \item  sentiment - Sentiment/polarity score
}
}
\description{
Approximate the sentiment (polarity) of text by sentence.

Calulate text polarity sentiment at the sentence level and optionally
aggregate by rows or grouping variable(s).
}
\details{
The equation used by the algorithm to assign value to polarity of
each sentence fist utilizes the sentiment dictionary (Hu and Liu, 2004) to
tag polarized words.  Each paragraph
(\eqn{p_i = \{s_1, s_2, ..., s_n\}}{p_i = \{s_1, s_2, ... s_n\}}) composed of
sentences, is broken into element sentences
(\eqn{s_i,j = \{w_1, w_2, ..., w_n\}}{s_i,j = \{w_1, w_2, ... w_n\}}) where \eqn{w}
are the words within sentences.  Each sentence (\eqn{s_j}) is broken into a
an ordered bag of words.  Punctuation is removed with the exception of pause
punctuations (commas, colons, semicolons) which are considered a word within
the sentence.  I will denote pause words as \eqn{cw} (comma words) for
convience.  We can represent these words as an i,j,k notation as
\eqn{w_{i,j,k}}.  For example \eqn{w_{325}} would be the fifth word of the
second sentence of the third paragraph.  While I use the term paragraph this
merely represent a complete turn of talk.  For example t may be a cell level
response in a questionare comprosed of sentences.

The words in each sentence (\eqn{w_{i,j,k}}) are searched and compared to a
modified version of Hu, M., & Liu, B.'s (2004) dictionary of polarized words.
Positive (\eqn{w_{i,j,k}^{+}}{w_i,j,k^+}) and negative
(\eqn{w_{i,j,k}^{-}}{w_i,j,k^-}) words are tagged with a \eqn{+1} and \eqn{-1}
respectively.  I will denote polarized words as \eqn{pw} for convience. These
will form a polar cluster (\eqn{c_{i,j,l}}{c_i,j,l}) which is a subset of the a
sentence (\eqn{c_{i,j,l} \subseteq s_i,j }{l_i,j,l \subseteq s_i,j}).

The polarized context cluster (\eqn{c_{i,j,l}}) of words is pulled from around
the polarized word (\eqn{pw}) and defaults to 4 words before and two words
after \eqn{pw}) to be considered as valence shifters.  The cluster can be represented as
(\eqn{c_{i,j,l} = \{pw_{i,j,k - nb}, ..., pw_{i,j,k} , ..., pw_{i,j,k - na}\}}{c_i,j,l = \{pw_i,j,k - nb, ..., pw_i,j,k , ..., pw_i,j,k - na\}}),
where \eqn{nb} & \eqn{na} are the parameters \code{n.before} and \code{n.after}
set by the user.  The words in this polarized context cluster are tagged as
neutral (\eqn{w_{i,j,k}^{0}}{w_i,j,k^0}), negator (\eqn{w_{i,j,k}^{n}}{w_i,j,k^n}),
amplifier (\eqn{w_{i,j,k}^{a}}{w_i,j,k^a}), or de-amplifier
(\eqn{w_{i,j,k}^{d}}{w_i,j,k^d}). Neutral words hold no value in the equation but
do affect word count (\eqn{n}).  Each polarized word is then weighted (\eqn{w})
based on the weights from the \code{polarity_dt} argument and then further
weighted by the function and number of the valence shifters directly surrounding the
positive or negative word (\eqn{pw}).  Pause (\eqn{cw}) locations
(punctuation that denotes a pause including commas, colons, and semicolons)
are indexed and considered in calculating the upper and lower bounds in the
polarized context cluster. This is because these marks indicate a change in
thought and words prior are not necessarily connected with words after these
punctuation marks.  The lower bound of the polarized context cluster is
constrained to \eqn{\max \{pw_{i,j,k - nb}, 1, \max \{cw_{i,j,k} < pw_{i,j,k}\}\}} and the upper bound is
constrained to \eqn{\min \{pw_{i,j,k + na}, w_{i,jn}, \min \{cw_{i,j,k} > pw_{i,j,k}\}\}}
where \eqn{w_{i,jn}} is the number of words in the sentence.

The core value in the cluster, the polarized word is acted uppon by valence
shifters. Amplifiers increas the polarity by 1.8 (.8 is the default weight
(\eqn{z})).  Amplifiers (\eqn{w_{i,j,k}^{a}}) become de-amplifiers if the clontext cluster
contains an odd number of negators (\eqn{w_{i,j,k}^{n}}).  De-amplifiers work
to decrease decrease the polarity.  Negation (\eqn{w_{i,j,k}^{n}}) acts on
amplifiers/de-amplifiers as discussed but also flip the sign of the polarized
word.  Negation is determined by raising -1 to the power of the number of
negators (\eqn{w_{i,j,k}^{n}}).  Simply, this is a result of a belief that two
negatives qual a positive, 3 negatives a negative and so on.

The researcher may provide a weight \eqn{z} to be utilized with
amplifiers/de-amplifiers (default is .8; de-amplifier weight is constrained
to -1 lower bound).  Last, these weighted context clusters (\eqn{c_{i,j,l}}{c_i,j,l}) are
summed (\eqn{c'_{i,j}}{c'_i,j}) and divided by the square root of the word count (\eqn{\sqrt{w_{i,jn}}}{\sqrtn w_i,jn}) yielding an unbounded
polarity score (\eqn{\delta}{C}) for each sentence.

\deqn{\delta=\frac{c'_{i,j}}{\sqrt{w_{i,jn}}}}{C=c'_i,j,l/\sqrt(w_i,jn)}

Where:

\deqn{c'_{i,j}=\sum{((1 + w_{amp} + w_{deamp})\cdot w_{i,j,k}^{p}(-1)^{\sum{w_{i,j,k}^{n}}})}}

\deqn{w_{amp}=\sum{(w_{neg}\cdot (z \cdot w_{i,j,k}^{a}))}}

\deqn{w_{deamp} = \max(w_{deamp'}, -1)}

\deqn{w_{deamp'}= \sum{(z(- w_{neg}\cdot w_{i,j,k}^{a} + w_{i,j,k}^{d}))}}

\deqn{w_{neg}= \left(\sum{w_{i,j,k}^{n}}\right) \bmod {2}}
}
\note{
The polarity score is dependent upon the polarity dictionary used.
This function defaults to the word polarity dictionary used by Hu, M., &
Liu, B. (2004), however, this may not be appropriate for the context of
children in a classroom.  The user may (is encouraged) to provide/augment the
dictionary (see the \code{sentiment_frame} function).  For instance the word
"sick" in a high school setting may mean that something is good, whereas
"sick" used by a typical adult indicates something is not right or negative
connotation (\strong{deixis}).
}
\examples{
mytext <- c(
   'do you like it?  But I hate really bad dogs',
   'I am the best friend.',
   'Do you really like it?  I\\'m not a fan'
)
sentiment(mytext)
sentiment(mytext, question.weight = 0)

sentiment(gsub("Sam-I-am", "Sam I am", sam_i_am))
}
\references{
Hu, M., & Liu, B. (2004). Mining opinion features in customer
reviews. National Conference on Artificial Intelligence.

\url{http://www.slideshare.net/jeffreybreen/r-by-example-mining-twitter-for}

\url{http://hedonometer.org/papers.html} Links to papers on hedonometrics
}
\seealso{
\url{https://github.com/trestletech/Sermon-Sentiment-Analysis}
}
\keyword{polarity}
\keyword{sentiment,}

